{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms,models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset,ConcatDataset\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"../input/digit-recognizer/train.csv\")\n",
    "test=pd.read_csv(\"../input/digit-recognizer/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, \n",
    "                 transform = transforms.Compose([transforms.ToTensor()])):\n",
    "        \n",
    "        df = dataframe\n",
    "        self.n_pixels = 784\n",
    "        \n",
    "        if len(df.columns) == self.n_pixels:\n",
    "            # validation data\n",
    "            self.X = df.values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = None\n",
    "        else:\n",
    "            # training data\n",
    "            self.X = df.iloc[:,1:].values.reshape((-1,28,28)).astype(np.uint8)[:,:,:,None]\n",
    "            self.y = torch.from_numpy(df.iloc[:,0].values)\n",
    "            \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.transform(self.X[idx]), self.y[idx]\n",
    "        else:\n",
    "            return self.transform(self.X[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " img_tform_1 = transforms.Compose([\n",
    "    transforms.ToPILImage(),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "img_tform_2 = transforms.Compose([\n",
    "    transforms.ToPILImage(),transforms.RandomRotation(10),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "img_tform_3 = transforms.Compose([\n",
    "    transforms.ToPILImage(),transforms.RandomRotation(20),transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "img_tform_4 = transforms.Compose([\n",
    "    transforms.ToPILImage(),transforms.RandomAffine(degrees=15, translate=(0.1,0.1), scale=(0.85,0.85)),\\\n",
    "    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "img_tform_5 = transforms.Compose([\n",
    "    transforms.ToPILImage(),transforms.RandomAffine(0,shear=30,scale=[1.15,1.15]),\\\n",
    "    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "img_tform_6 = transforms.Compose([\n",
    "    transforms.ToPILImage(),transforms.RandomAffine(0,shear=20,scale=[0.8,0.8]),\\\n",
    "    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n",
    "\n",
    "img_tform_7 = transforms.Compose([\n",
    "    transforms.ToPILImage(),transforms.RandomAffine(degrees=30, scale=(1.2,1.2)),\\\n",
    "    transforms.ToTensor(),transforms.Normalize((0.5),(0.5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed=42\n",
    "def create_dataloaders(seed, test_size=0.1, df=train, batch_size=64):\n",
    "    # Create training set and validation set\n",
    "    train_df, val_df = train_test_split(df,test_size=test_size,random_state=seed)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_data_1 = MnistDataset(train_df)\n",
    "    train_data_2 = MnistDataset(train_df, img_tform_2)\n",
    "    train_data_3 = MnistDataset(train_df, img_tform_3)\n",
    "    train_data_4 = MnistDataset(train_df, img_tform_4)\n",
    "    train_data_5 = MnistDataset(train_df, img_tform_5)\n",
    "    train_data_6 = MnistDataset(train_df, img_tform_6)\n",
    "    train_data_7 = MnistDataset(train_df, img_tform_7)\n",
    "    train_final = ConcatDataset([train_data_1, train_data_2, train_data_3, train_data_4, train_data_5,\\\n",
    "                                   train_data_6,train_data_7])\n",
    "\n",
    "    val_data = MnistDataset(val_df)\n",
    "    \n",
    "    # Create Dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_final, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, valid_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(pretrained=True)\n",
    "model.conv1 = torch.nn.Conv1d(1, 64, (3, 3), (1, 1), (1, 1), bias=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),amsgrad=True)\n",
    "xp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 20\n",
    "valid_loss_min = np.Inf\n",
    "train_epoch=[]\n",
    "train_loss_vals=[]\n",
    "train_acc_vals=[]\n",
    "valid_epoch=[]\n",
    "valid_loss_vals=[]\n",
    "valid_acc_vals=[]\n",
    "test_loss_val=[]\n",
    "test_epoch=[]\n",
    "train_loader, valid_loader= create_dataloaders(seed=seed)\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    train_acc=0\n",
    "    valid_acc=0\n",
    "    total=0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for data, target in tepoch:\n",
    "            if torch.cuda.is_available():\n",
    "                data,target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output=model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            train_acc+=((predicted==target).sum().item())\n",
    "            total += target.size(0)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            train_epoch.append(loss.item())\n",
    "            optimizer.step()\n",
    "        \n",
    "    xp_lr_scheduler.step()\n",
    "    train_loss_vals.append(sum(train_epoch)/len(train_epoch))\n",
    "    train_acc_vals.append(100 * train_acc/ total)\n",
    "    model.eval()\n",
    "    total=0\n",
    "    with tqdm(valid_loader, unit=\"batch\") as tepoch:\n",
    "        for data, target in tepoch:\n",
    "            if torch.cuda.is_available():\n",
    "                data,target= data.cuda(),target.cuda()\n",
    "            output=model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            valid_acc+=((predicted==target).sum().item())\n",
    "            total += target.size(0)\n",
    "            loss= criterion(output,target)\n",
    "            valid_epoch.append(loss.item())\n",
    "    valid_loss_vals.append(sum(valid_epoch)/len(valid_epoch))\n",
    "    valid_acc_vals.append(100 * valid_acc/ total)\n",
    "    \n",
    "    print(\"epoch:{}\\t  training_loss:{}\\t  validation_loss:{}\\t  train_accuracy:{}\\t  validation_accuracy:{}\"\n",
    "          .format(i,train_loss_vals[i],valid_loss_vals[i],train_acc_vals[i],valid_acc_vals[i]))\n",
    "    if valid_loss_vals[i] <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss_vals[i]))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss_vals[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_cifar.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, epochs, epochs).astype(int), train_acc_vals,label='train_accuracy')\n",
    "plt.plot(np.linspace(1, epochs, epochs).astype(int), valid_acc_vals,label='valid_accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1, epochs, epochs).astype(int), train_loss_vals,label='train_loss')\n",
    "plt.plot(np.linspace(1, epochs, epochs).astype(int), valid_loss_vals,label='valid_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title('loss functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_images = test.values.reshape((-1, 1, 28, 28)) / 255.0\n",
    "print(test_images.shape)\n",
    "\n",
    "test_image_tensor = torch.tensor(test_images, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "result = np.zeros(test_images.shape[0], dtype=np.int64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(test_images.shape[0]):\n",
    "        image = test_image_tensor[i, 0, :, :].view(1, 1, 28, 28)\n",
    "        output=model(image.cuda())\n",
    "        _, pred = torch.max(output, 1) \n",
    "        result[i] = classes[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission=pd.read_csv('../input/digit-recognizer/sample_submission.csv')\n",
    "sample_submission['Label']=result\n",
    "sample_submission.to_csv('submission1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63963b3f4c440940f0b94a3100916033a226cb4f45979123153792d60aa56d6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
